{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleExample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulKHpLRCBJgl"
      },
      "source": [
        "# This code contains a simple example using the function `PCInfer` from `Torch2PC` for training a convolutional neural network on MNIST.\n",
        "\n",
        "The first code cell imports the MNIST data and defines some hyperparameters, but contains nothing specific to `Torch2PC`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcEy5twAUPkq"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision \n",
        "import matplotlib.pyplot as plt\n",
        "from time import time as tm\n",
        "\n",
        "# Import TorchSeq2PC \n",
        "!git clone https://github.com/RobertRosenbaum/Torch2PC.git\n",
        "from Torch2PC import TorchSeq2PC as T2PC  \n",
        "\n",
        "# Seed rng\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# # This patches an error that sometimes arises in\n",
        "# # downloading MNIST\n",
        "# from six.moves import urllib\n",
        "# opener = urllib.request.build_opener()\n",
        "# opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
        "# urllib.request.install_opener(opener)\n",
        "\n",
        "# This seems to be a more reliable and faster\n",
        "# source for MNIST\n",
        "!wget -nc www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "# Load training and testing data from MNIST dataset\n",
        "# These lines return data structures that contain\n",
        "# the training and testing data \n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Get training data structure\n",
        "train_dataset = MNIST('./', \n",
        "      train=True, \n",
        "      transform=torchvision.transforms.ToTensor(),  \n",
        "      download=True)\n",
        "\n",
        "# Number of trainin data points\n",
        "m = len(train_dataset)\n",
        "\n",
        "# Print the size of the training data set\n",
        "print('\\n\\n\\n')\n",
        "print(\"Number of data points in training set = \",m)\n",
        "print(\"Size of training inputs (X)=\",train_dataset.data.size())\n",
        "print(\"Size of training labels (Y)=\",train_dataset.targets.size())\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 300      # Batch size to use with training data\n",
        "\n",
        "# Create data loader. \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "\n",
        "# Choose device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device = ',device)\n",
        "\n",
        "# Define the nunmber of epochs, learning rate, \n",
        "# and how often to print progress\n",
        "num_epochs=2\n",
        "LearningRate=.002\n",
        "PrintEvery=50\n",
        "\n",
        "# Choose an optimizer\n",
        "WhichOptimizer=torch.optim.Adam\n",
        "\n",
        "# Compute size of each batch\n",
        "steps_per_epoch = len(train_loader) \n",
        "total_num_steps  = num_epochs*steps_per_epoch\n",
        "print(\"steps per epoch (mini batch size)=\",steps_per_epoch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fxLn1PGBlTY"
      },
      "source": [
        "# The next code cell builds a convolutional neural network model using `Sequential`. \n",
        "\n",
        "The `PCInfer` function treats each element of a `Sequential` model as a layer. As such, it is necessary to use nested calls to `Sequential` (as below) if you want to treat a block of functions as a layer. For the model below, each block will be treated as a layer (5 layers in all)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrqeO3VeURXa"
      },
      "source": [
        "\n",
        "# Define model using Sequential. \n",
        "model=nn.Sequential(\n",
        "    \n",
        "    nn.Sequential(nn.Conv2d(1,10,3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2)\n",
        "    ),\n",
        "\n",
        "    nn.Sequential(\n",
        "    nn.Conv2d(10,5,3),\n",
        "    nn.ReLU(),\n",
        "    nn.Flatten()\n",
        "    ),\n",
        "\n",
        " nn.Sequential(    \n",
        "    nn.Linear(5*11*11,50),\n",
        "    nn.ReLU()\n",
        "    ),\n",
        "\n",
        " nn.Sequential(    \n",
        "    nn.Linear(50,30),\n",
        "    nn.ReLU()\n",
        "    ),\n",
        "\n",
        "\n",
        "nn.Sequential(\n",
        "   nn.Linear(30,10)\n",
        " )\n",
        "\n",
        ").to(device)\n",
        "\n",
        "# Define the loss function\n",
        "LossFun = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute one batch of output and loss to make sure\n",
        "# things are working\n",
        "with torch.no_grad():\n",
        "  TrainingIterator=iter(train_loader)\n",
        "  X,Y=next(TrainingIterator)  \n",
        "  X=X.to(device)\n",
        "  Y=Y.to(device)\n",
        "  Yhat=model(X).to(device)\n",
        "  print('output shape = ',Yhat.shape)\n",
        "  print('loss on initial model = ',LossFun(Yhat,Y).item())\n",
        "\n",
        "\n",
        "NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Number of trainable parameters in model =',NumParams)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfl9P_A6CLa8"
      },
      "source": [
        "# The next code cell defines hyperparameters for `PCInfer`\n",
        "\n",
        "The hyperparameter `ErrType` controls which algorithm to use for computing the beliefs and prediction errors. It should be equal to `'Strict'`, `'FixedPred'`, or `'Exact'`. `'Strict'` uses a strict interpretation of predictive coding (without the fixed prediction assumption), `'FixedPred'` uses the fixed prediction assumption, and `'Exact'` computes the exact gradients (same as those computed by backpropagation). See \"On the relationship between predictive coding and backpropagation\" for more information on these algorithms.\n",
        "\n",
        "`eta` and `n` are the step size and number of steps to use for the iterations that compute the prediction errors and beliefs. These parameters are not used when `ErrType='Exact'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYYDZD6iN-dO"
      },
      "source": [
        "# Define PC hyperparameters\n",
        "\n",
        "ErrType=\"Strict\"\n",
        "eta=.1\n",
        "n=20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm0Z6rK-Cmua"
      },
      "source": [
        "# The next code cell uses `PCInfer` to train the model\n",
        "\n",
        "The only line that differs from a typical training loop in PyTorch is the line\n",
        "\n",
        "`\n",
        "vhat,Loss,dLdy,v,epsilon=T2PC.PCInfer(model,LossFun,X,Y,ErrType,eta,n)\n",
        "`\n",
        "\n",
        "which computes the outputs, loss, etc. and it sets the `.grad` attributes of all parameters in `model` to the parameter update values computed by predictive coding. \n",
        "\n",
        "For `ErrType='Exact'`, the gradients are set to the gradient of the loss with respect to that parameter, i.e., the same values computed by calling `Loss.backward()` after a single forward pass. For other values of `ErrType`, refer to the paper for an explanation of how the parameter updates are computed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBqT3hYHUXAC"
      },
      "source": [
        "\n",
        "# Define the optimizer\n",
        "optimizer = WhichOptimizer(model.parameters(), lr=LearningRate)\n",
        "\n",
        "# Initialize vector to store losses\n",
        "LossesToPlot=np.zeros(total_num_steps)\n",
        "\n",
        "\n",
        "j=0     # Counters\n",
        "jj=0    \n",
        "t1=tm() # Get start time\n",
        "for k in range(num_epochs):\n",
        "\n",
        "  # Re-initialize the training iterator (shuffles data for one epoch)\n",
        "  TrainingIterator=iter(train_loader)\n",
        "  \n",
        "  for i in range(steps_per_epoch): # For each batch\n",
        "\n",
        "    # Get one batch of training data, reshape it\n",
        "    # and send it to the current device        \n",
        "    X,Y=next(TrainingIterator)  \n",
        "    X=X.to(device)\n",
        "    Y=Y.to(device)\n",
        "\n",
        "    # Perform inference on this batch\n",
        "    vhat,Loss,dLdy,v,epsilon=T2PC.PCInfer(model,LossFun,X,Y,ErrType,eta,n)\n",
        "\n",
        "    # Update parameters    \n",
        "    optimizer.step() \n",
        "\n",
        "    # Zero-out gradients     \n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Print and store loss\n",
        "    with torch.no_grad():\n",
        "      if(i%PrintEvery==0):\n",
        "        print('epoch =',k,'step =',i,'Loss =',Loss.item())\n",
        "      LossesToPlot[jj]=Loss.item() \n",
        "      jj+=1\n",
        "\n",
        "# Compute and print time spent training\n",
        "tTrain=tm()-t1\n",
        "print('Training time = ',tTrain,'sec')\n",
        "\n",
        "# Plot the loss curve\n",
        "plt.figure()\n",
        "plt.plot(LossesToPlot)\n",
        "plt.ylim(bottom=0)  \n",
        "plt.ylabel('training loss')\n",
        "plt.xlabel('iteration number')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}